{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class MatrixEnvironment:\n",
    "    def __init__(self, m, n, p):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.state = torch.zeros(self.m, self.n, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action_matrix):\n",
    "        self.state = action_matrix\n",
    "        # random_vector = torch.tensor(np.random.choice([0, 1], size=(self.m,), p=[1-self.p, self.p]), dtype=torch.float32, requires_grad=True)\n",
    "        reward = self.calculate_reward(action_matrix)\n",
    "        done = True  # Episode is done after one step in this simple environment\n",
    "        return reward\n",
    "\n",
    "    def calculate_reward(self, X, lost_penalty=40, a=1, l=0, o=100, s=1):\n",
    "        m_nr, t_nr = X.shape\n",
    "        m = torch.tensor(np.ones(t_nr)*self.p, dtype=torch.float32, requires_grad=True)\n",
    "        t = torch.tensor(np.ones(t_nr), dtype=torch.float32, requires_grad=True) #if t is None else t\n",
    "        mm = torch.zeros(t_nr, dtype=torch.float32)\n",
    "        for tag in range(t_nr):\n",
    "            mask = m[X[:, tag] == 1]  # mask for tag\n",
    "            mm[tag] = torch.prod(mask)  # multiply the mask\n",
    "        A = torch.matmul(X, (mm * t))\n",
    "        # A = torch.where(A > 1, torch.ones_like(A), A)\n",
    "\n",
    "        # L = [torch.where(X[:, torch.where((X[msg, :] * (mm * t)) > 0)[0][0]] == 1)[0][-1].item() - msg if A[msg] > 0 else lost_penalty for msg in range(m_nr)]\n",
    "        # L = torch.tensor(L, dtype=torch.float32)\n",
    "\n",
    "        return a * torch.sum(A) #+ l * torch.sum(L)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedPolicyNetwork(nn.Module):\n",
    "    def __init__(self, m, n):\n",
    "        super(FullyConnectedPolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(m * n, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, m * n)\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x.view(self.m, self.n)\n",
    "    \n",
    "\n",
    "\n",
    "class DeepPolicyNetworkCNN(nn.Module):\n",
    "    def __init__(self, m, n):\n",
    "        super(DeepPolicyNetworkCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * m * n, 128)\n",
    "        self.fc2 = nn.Linear(128, m * n)\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(-1, 64 * self.m * self.n)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.view(self.m, self.n)\n",
    "\n",
    "\n",
    "# def gumbel_softmax(logits, tau=1, hard=False):\n",
    "#     gumbels = -torch.empty_like(logits).exponential_().log()  # ~Gumbel(0,1)\n",
    "#     gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
    "#     y_soft = gumbels.softmax(dim=-1)\n",
    "\n",
    "#     if hard:\n",
    "#         # Straight through.\n",
    "#         index = y_soft.max(dim=-1, keepdim=True)[1]\n",
    "#         y_hard = torch.zeros_like(logits).scatter_(-1, index, 1.0)\n",
    "#         ret = y_hard - y_soft.detach() + y_soft\n",
    "#     else:\n",
    "#         # Reparameterization trick.\n",
    "#         ret = y_soft\n",
    "#     return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example reward function usage\n",
    "# m = torch.tensor([1, 1, 0, 1], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "m, n = 4, 4\n",
    "p = 0.9\n",
    "\n",
    "\n",
    "X = torch.tensor([[1, 0, 0, 0],\n",
    "                  [1, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "env = MatrixEnvironment(m, n, p)\n",
    "policy_net = FullyConnectedPolicyNetwork(m, n)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=.001)\n",
    "\n",
    "# env.step(X)\n",
    "    \n",
    "for episode in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    X = env.get_state()\n",
    "    X.retain_grad()\n",
    "\n",
    "    r= env.step(X)\n",
    "    loss = -r\n",
    "    r.backward(retain_graph= True)\n",
    "    # print(X.grad)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    X = policy_net(X)\n",
    "    X = torch.bernoulli(X)\n",
    "    X.retain_grad()\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
