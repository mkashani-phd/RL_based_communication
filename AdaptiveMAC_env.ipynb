{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from pulp import LpProblem, LpMinimize, LpVariable, lpSum, PULP_CBC_CMD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# based on mask and multiply operation\n",
    "def Validate( X,m,t,rectified = False, includeValidTag = False):\n",
    "    # mask and multiply\n",
    "    X = np.array(X,dtype=int)\n",
    "    m_nr,t_nr = X.shape\n",
    "    mm = np.zeros(t_nr)\n",
    "    for tag in range(t_nr):\n",
    "        mask = m[np.where(X[:,tag] == 1)] # mask for tag\n",
    "        mm[tag] = np.prod(mask) # multiply the mask\n",
    "    A = np.matmul(X,(mm*t).transpose())\n",
    "\n",
    "    A = np.array( [1 if x > 1 else x for x in A]) if rectified else A\n",
    "    return (A, mm*t) if includeValidTag else A\n",
    "\n",
    "\n",
    "def Latency( X,m,t,lost_penalty = 40):\n",
    "    X = np.array(X,dtype=int)\n",
    "    m_nr,t_nr = X.shape\n",
    "    A,validTags = Validate(X,m,t,includeValidTag=True)\n",
    "    L = [np.where(X[:,np.where((X[msg,:]*validTags)>0)[0][0]] == 1)[0][-1]-msg if A[msg] >0 else lost_penalty for msg in range(m_nr)]\n",
    "\n",
    "    return np.array(L)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def  Get_Strength_Number(X):\n",
    "    X = np.array(X,dtype=int)\n",
    "\n",
    "    n_msg, n_tag = X.shape\n",
    "    A = np.transpose(X)\n",
    "    B = np.ones(n_tag)\n",
    "    C = np.ones(n_msg)\n",
    "\n",
    "    prob = LpProblem(\"Binary_LP_Problem\", LpMinimize)\n",
    "\n",
    "    # Define the variables\n",
    "    x = [LpVariable(f'x{i}', cat='Binary') for i in range(len(C))]\n",
    "\n",
    "    # Define the objective function\n",
    "    prob += lpSum(C[i] * x[i] for i in range(len(C)))\n",
    "\n",
    "    # Define the constraints\n",
    "    for i in range(len(A)):\n",
    "        prob += lpSum(A[i][j] * x[j] for j in range(len(C))) >= B[i]\n",
    "\n",
    "    # Solve the problem\n",
    "    prob.solve(PULP_CBC_CMD(msg=0))\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Status:\", prob.status)\n",
    "    print(\"Objective value:\", prob.objective.value())\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(C)):\n",
    "        #print(f\"x{i+1}:\", x[i].value())\n",
    "        if x[i].value() == 1:\n",
    "            result.append(i+1)\n",
    "    return len(result)\n",
    "\n",
    "def Reward(X,m,t,rectified_A = False, lost_penalty = 40, a = 1,l = 1, o = 100, s=1):\n",
    "    X = np.array(X,dtype=int)\n",
    "    A = Validate(X,m,t,rectified=rectified_A)\n",
    "    L = Latency(X,A,t,lost_penalty=lost_penalty)\n",
    "\n",
    "    m_nr,t_nr = X.shape\n",
    "    r = a*np.sum(A) - l*np.sum(L) - o * t_nr/m_nr + s*Get_Strength_Number(X) \n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "\n",
    "class MatrixEnvironment:\n",
    "    def __init__(self, m, n, p):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.choice([0, 1], size=(self.m,), p=[1-self.p, self.p])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action_matrix):\n",
    "        random_vector = np.random.choice([0, 1], size=(self.m,), p=[1-self.p, self.p])\n",
    "        reward = self.calculate_reward(random_vector, action_matrix)\n",
    "        done = True  # Episode is done after one step in this simple environment\n",
    "        return random_vector, reward, done\n",
    "\n",
    "    def calculate_reward(self, random_vector, action_matrix):\n",
    "        # User-defined reward function\n",
    "        reward = Reward(X = action_matrix, m = random_vector, t = np.ones(self.n) , rectified_A=False, lost_penalty=40, a=1, l=1, o=100, s=1)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, m, n):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(m * n, 128)\n",
    "        self.fc2 = nn.Linear(128, m * n)\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.view(self.m, self.n)\n",
    "\n",
    "# Example usage\n",
    "m, n = 100, 100\n",
    "p = 0.99\n",
    "env = MatrixEnvironment(m, n, p)\n",
    "policy_net = PolicyNetwork(m, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 1\n",
      "Objective value: 4.0\n",
      "Episode 0, Loss: 0.49239999055862427\n",
      "Status: 1\n",
      "Objective value: 3.0\n",
      "Status: 1\n",
      "Objective value: 4.0\n",
      "Status: 1\n",
      "Objective value: 4.0\n",
      "Status: 1\n",
      "Objective value: 4.0\n",
      "Status: 1\n",
      "Objective value: 3.0\n",
      "Status: 1\n",
      "Objective value: 4.0\n",
      "Status: 1\n",
      "Objective value: 4.0\n",
      "Status: 1\n",
      "Objective value: 4.0\n"
     ]
    }
   ],
   "source": [
    "def train(env, policy_net, num_episodes=1000, learning_rate=0.01):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()  # Using MSE as a placeholder; adapt as needed for the reward structure\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Generate an action matrix from the policy network\n",
    "        action_matrix = torch.bernoulli(policy_net(torch.zeros(env.m, env.n)))\n",
    "        \n",
    "        random_vector, reward, done = env.step(action_matrix.detach().numpy())\n",
    "        \n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        \n",
    "        loss = loss_fn(action_matrix, torch.zeros(env.m, env.n))  # Placeholder loss; adapt based on reward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Loss: {loss.item()}\")\n",
    "\n",
    "train(env, policy_net)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
